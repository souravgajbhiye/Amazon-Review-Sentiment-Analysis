{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "#Metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # The objective of this project is to build a model that can create relevant summaries for reviews written about fine foods sold on Amazon. This dataset contains above 5,00,000 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568454, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Reviews.csv\")\n",
    "data =  df.copy()\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                         0\n",
       "ProductId                  0\n",
       "UserId                     0\n",
       "ProfileName               16\n",
       "HelpfulnessNumerator       0\n",
       "HelpfulnessDenominator     0\n",
       "Score                      0\n",
       "Time                       0\n",
       "Summary                   27\n",
       "Text                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Score[data.Score<=3]=0\n",
    "data.Score[data.Score>=4]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568449</th>\n",
       "      <td>568450</td>\n",
       "      <td>B001EO7N10</td>\n",
       "      <td>A28KG5XORO54AY</td>\n",
       "      <td>Lettie D. Carter</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1299628800</td>\n",
       "      <td>Will not do without</td>\n",
       "      <td>Great for sesame chicken..this is a good if no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568450</th>\n",
       "      <td>568451</td>\n",
       "      <td>B003S1WTCU</td>\n",
       "      <td>A3I8AFVPEE8KI5</td>\n",
       "      <td>R. Sawyer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1331251200</td>\n",
       "      <td>disappointed</td>\n",
       "      <td>I'm disappointed with the flavor. The chocolat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568451</th>\n",
       "      <td>568452</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A121AA1GQV751Z</td>\n",
       "      <td>pksd \"pk_007\"</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1329782400</td>\n",
       "      <td>Perfect for our maltipoo</td>\n",
       "      <td>These stars are small, so you can give 10-15 o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568452</th>\n",
       "      <td>568453</td>\n",
       "      <td>B004I613EE</td>\n",
       "      <td>A3IBEVCTXKNOH</td>\n",
       "      <td>Kathy A. Welch \"katwel\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1331596800</td>\n",
       "      <td>Favorite Training and reward treat</td>\n",
       "      <td>These are the BEST treats for training and rew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568453</th>\n",
       "      <td>568454</td>\n",
       "      <td>B001LR2CU2</td>\n",
       "      <td>A3LGQPJCZVL9UC</td>\n",
       "      <td>srfell17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1338422400</td>\n",
       "      <td>Great Honey</td>\n",
       "      <td>I am very satisfied ,product is as advertised,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>568452 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId                      ProfileName  \\\n",
       "0            1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1            2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2            3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3            4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4            5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "...        ...         ...             ...                              ...   \n",
       "568449  568450  B001EO7N10  A28KG5XORO54AY                 Lettie D. Carter   \n",
       "568450  568451  B003S1WTCU  A3I8AFVPEE8KI5                        R. Sawyer   \n",
       "568451  568452  B004I613EE  A121AA1GQV751Z                    pksd \"pk_007\"   \n",
       "568452  568453  B004I613EE   A3IBEVCTXKNOH          Kathy A. Welch \"katwel\"   \n",
       "568453  568454  B001LR2CU2  A3LGQPJCZVL9UC                         srfell17   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                          1                       1      1  1303862400   \n",
       "1                          0                       0      0  1346976000   \n",
       "2                          1                       1      1  1219017600   \n",
       "3                          3                       3      0  1307923200   \n",
       "4                          0                       0      1  1350777600   \n",
       "...                      ...                     ...    ...         ...   \n",
       "568449                     0                       0      1  1299628800   \n",
       "568450                     0                       0      0  1331251200   \n",
       "568451                     2                       2      1  1329782400   \n",
       "568452                     1                       1      1  1331596800   \n",
       "568453                     0                       0      1  1338422400   \n",
       "\n",
       "                                   Summary  \\\n",
       "0                    Good Quality Dog Food   \n",
       "1                        Not as Advertised   \n",
       "2                    \"Delight\" says it all   \n",
       "3                           Cough Medicine   \n",
       "4                              Great taffy   \n",
       "...                                    ...   \n",
       "568449                 Will not do without   \n",
       "568450                        disappointed   \n",
       "568451            Perfect for our maltipoo   \n",
       "568452  Favorite Training and reward treat   \n",
       "568453                         Great Honey   \n",
       "\n",
       "                                                     Text  \n",
       "0       I have bought several of the Vitality canned d...  \n",
       "1       Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2       This is a confection that has been around a fe...  \n",
       "3       If you are looking for the secret ingredient i...  \n",
       "4       Great taffy at a great price.  There was a wid...  \n",
       "...                                                   ...  \n",
       "568449  Great for sesame chicken..this is a good if no...  \n",
       "568450  I'm disappointed with the flavor. The chocolat...  \n",
       "568451  These stars are small, so you can give 10-15 o...  \n",
       "568452  These are the BEST treats for training and rew...  \n",
       "568453  I am very satisfied ,product is as advertised,...  \n",
       "\n",
       "[568452 rows x 10 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = data[data.HelpfulnessNumerator <= data.HelpfulnessDenominator]\n",
    "print(final.shape)\n",
    "final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD4CAYAAAAQP7oXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADZhJREFUeJzt3X+onuV9x/H316TphNLGmlNxSbYT8MAaC+uPEAP9Z+gwJ3Us/lEhMmaQQKBEaOlgxv0jbSfoP3MIVghL1jhG09ANDDZdCFEZY1VzXJ0uBpez1DWHiDk20VlKddHv/niu6LPjc87zTZblPvG8X/Dw3Pf3uu77ug488Ml939fzJDITSZIqruh6ApKky4ehIUkqMzQkSWWGhiSpzNCQJJUZGpKkMkNDklRmaEiSygwNSVLZ4q4ncLEtW7YsR0dHu56GJF1Wnnvuudczc2RYv49caIyOjjIxMdH1NCTpshIR/1np5+0pSVKZoSFJKjM0JEllhoYkqczQkCSVGRqSpDJDQ5JUZmhIkso+cl/uu1yMbv9R11P4yHjl/lu6noK0YHilIUkqMzQkSWWGhiSpzNCQJJUZGpKkMkNDklRmaEiSygwNSVKZoSFJKjM0JEllhoYkqczQkCSVGRqSpDJDQ5JUZmhIksoMDUlSWTk0ImJRRPw0Ih5v+6si4pmIOBYRP4iIJa3+8bY/2dpH+85xT6u/HBHr++rjrTYZEdv76gPHkCR143yuNL4OHO3bfwB4MDPHgDPAllbfApzJzOuAB1s/ImI1sAm4HhgHvtuCaBHwMLABWA3c3vrONYYkqQOl0IiIFcAtwF+1/QBuBH7YuuwGbm3bG9s+rf2m1n8jsCcz387MnwGTwNr2mszM45n5DrAH2DhkDElSB6pXGn8J/CnwXtu/GngjM8+2/SlgedteDpwAaO1vtv7v12ccM1t9rjEkSR0YGhoR8QfAqcx8rr88oGsOabtY9UFz3BoRExExMT09PaiLJOkiqFxpfBn4w4h4hd6toxvpXXksjYjFrc8K4GTbngJWArT2TwGn++szjpmt/vocY/wvmbkjM9dk5pqRkZHCnyRJuhBDQyMz78nMFZk5Su9B9hOZ+UfAk8BXW7fNwGNte1/bp7U/kZnZ6pva6qpVwBjwLHAYGGsrpZa0Mfa1Y2YbQ5LUgf/L9zTuBr4ZEZP0nj/sbPWdwNWt/k1gO0BmHgH2Ai8B/wBsy8x32zOLu4AD9FZn7W195xpDktSBxcO7fCAznwKeatvH6a18mtnn18Btsxx/H3DfgPp+YP+A+sAxJEnd8BvhkqQyQ0OSVGZoSJLKDA1JUpmhIUkqMzQkSWWGhiSpzNCQJJUZGpKkMkNDklRmaEiSygwNSVKZoSFJKjM0JEllhoYkqczQkCSVGRqSpDJDQ5JUZmhIksoMDUlSmaEhSSozNCRJZYaGJKnM0JAklRkakqQyQ0OSVGZoSJLKDA1JUpmhIUkqMzQkSWWGhiSpzNCQJJUZGpKkMkNDklRmaEiSygwNSVKZoSFJKjM0JEllQ0MjIn4jIp6NiH+NiCMR8a1WXxURz0TEsYj4QUQsafWPt/3J1j7ad657Wv3liFjfVx9vtcmI2N5XHziGJKkblSuNt4EbM/N3gc8D4xGxDngAeDAzx4AzwJbWfwtwJjOvAx5s/YiI1cAm4HpgHPhuRCyKiEXAw8AGYDVwe+vLHGNIkjowNDSy55dt92PtlcCNwA9bfTdwa9ve2PZp7TdFRLT6nsx8OzN/BkwCa9trMjOPZ+Y7wB5gYztmtjEkSR0oPdNoVwTPA6eAg8B/AG9k5tnWZQpY3raXAycAWvubwNX99RnHzFa/eo4xZs5va0RMRMTE9PR05U+SJF2AUmhk5ruZ+XlgBb0rg88O6tbeY5a2i1UfNL8dmbkmM9eMjIwM6iJJugjOa/VUZr4BPAWsA5ZGxOLWtAI42bangJUArf1TwOn++oxjZqu/PscYkqQOVFZPjUTE0rZ9JfD7wFHgSeCrrdtm4LG2va/t09qfyMxs9U1tddUqYAx4FjgMjLWVUkvoPSzf146ZbQxJUgcWD+/CtcDutsrpCmBvZj4eES8BeyLiz4GfAjtb/53A30TEJL0rjE0AmXkkIvYCLwFngW2Z+S5ARNwFHAAWAbsy80g7192zjCFJ6sDQ0MjMF4AvDKgfp/d8Y2b918Bts5zrPuC+AfX9wP7qGJKkbviNcElSmaEhSSozNCRJZYaGJKnM0JAklRkakqQyQ0OSVGZoSJLKDA1JUpmhIUkqMzQkSWWGhiSpzNCQJJUZGpKkMkNDklRmaEiSygwNSVKZoSFJKjM0JEllhoYkqczQkCSVGRqSpDJDQ5JUZmhIksoMDUlSmaEhSSozNCRJZYaGJKnM0JAklRkakqQyQ0OSVGZoSJLKDA1JUpmhIUkqMzQkSWWGhiSpzNCQJJUZGpKksqGhERErI+LJiDgaEUci4uut/umIOBgRx9r7Va0eEfFQRExGxAsR8cW+c21u/Y9FxOa++pci4sV2zEMREXONIUnqRuVK4yzwJ5n5WWAdsC0iVgPbgUOZOQYcavsAG4Cx9toKPAK9AADuBW4A1gL39oXAI63vuePGW322MSRJHRgaGpn5amb+S9t+CzgKLAc2Artbt93ArW17I/Bo9jwNLI2Ia4H1wMHMPJ2ZZ4CDwHhr+2Rm/iQzE3h0xrkGjSFJ6sB5PdOIiFHgC8AzwDWZ+Sr0ggX4TOu2HDjRd9hUq81VnxpQZ44xZs5ra0RMRMTE9PT0+fxJkqTzUA6NiPgE8HfANzLzv+bqOqCWF1Avy8wdmbkmM9eMjIycz6GSpPNQCo2I+Bi9wPjbzPz7Vn6t3VqivZ9q9SlgZd/hK4CTQ+orBtTnGkOS1IHK6qkAdgJHM/Mv+pr2AedWQG0GHuur39FWUa0D3my3lg4AN0fEVe0B+M3Agdb2VkSsa2PdMeNcg8aQJHVgcaHPl4E/Bl6MiOdb7c+A+4G9EbEF+DlwW2vbD3wFmAR+BdwJkJmnI+I7wOHW79uZebptfw34HnAl8OP2Yo4xJEkdGBoamflPDH7uAHDTgP4JbJvlXLuAXQPqE8DnBtR/MWgMSVI3/Ea4JKnM0JAklRkakqQyQ0OSVGZoSJLKDA1JUpmhIUkqMzQkSWWGhiSpzNCQJJUZGpKkMkNDklRmaEiSygwNSVKZoSFJKjM0JEllhoYkqczQkCSVGRqSpDJDQ5JUZmhIksoMDUlSmaEhSSozNCRJZYaGJKnM0JAklRkakqQyQ0OSVGZoSJLKFnc9AUnzy+j2H3U9hY+UV+6/pespXFReaUiSygwNSVKZoSFJKjM0JEllhoYkqczQkCSVGRqSpLKhoRERuyLiVET8W1/t0xFxMCKOtferWj0i4qGImIyIFyLii33HbG79j0XE5r76lyLixXbMQxERc40hSepO5Urje8D4jNp24FBmjgGH2j7ABmCsvbYCj0AvAIB7gRuAtcC9fSHwSOt77rjxIWNIkjoyNDQy8x+B0zPKG4HdbXs3cGtf/dHseRpYGhHXAuuBg5l5OjPPAAeB8db2ycz8SWYm8OiMcw0aQ5LUkQt9pnFNZr4K0N4/0+rLgRN9/aZaba761ID6XGNIkjpysR+Ex4BaXkD9/AaN2BoRExExMT09fb6HS5KKLjQ0Xmu3lmjvp1p9CljZ128FcHJIfcWA+lxjfEhm7sjMNZm5ZmRk5AL/JEnSMBcaGvuAcyugNgOP9dXvaKuo1gFvtltLB4CbI+Kq9gD8ZuBAa3srIta1VVN3zDjXoDEkSR0Z+tPoEfF94PeAZRExRW8V1P3A3ojYAvwcuK113w98BZgEfgXcCZCZpyPiO8Dh1u/bmXnu4frX6K3QuhL4cXsxxxiSpI4MDY3MvH2WppsG9E1g2yzn2QXsGlCfAD43oP6LQWNIkrrjN8IlSWWGhiSpzNCQJJUZGpKkMkNDklRmaEiSygwNSVKZoSFJKjM0JEllhoYkqczQkCSVGRqSpDJDQ5JUZmhIksoMDUlSmaEhSSozNCRJZYaGJKnM0JAklRkakqQyQ0OSVGZoSJLKDA1JUpmhIUkqMzQkSWWGhiSpzNCQJJUZGpKkMkNDklRmaEiSygwNSVKZoSFJKjM0JEllhoYkqczQkCSVGRqSpDJDQ5JUZmhIksrmfWhExHhEvBwRkxGxvev5SNJCNq9DIyIWAQ8DG4DVwO0RsbrbWUnSwjWvQwNYC0xm5vHMfAfYA2zseE6StGAt7noCQywHTvTtTwE3zOwUEVuBrW33lxHx8iWY20KxDHi960nMJR7oegbqyLz/bMJl9fn87Uqn+R4aMaCWHypk7gB2/P9PZ+GJiInMXNP1PKSZ/Gx2Y77fnpoCVvbtrwBOdjQXSVrw5ntoHAbGImJVRCwBNgH7Op6TJC1Y8/r2VGaejYi7gAPAImBXZh7peFoLjbf9NF/52exAZH7oEYEkSQPN99tTkqR5xNCQJJUZGpKksnn9IFySzomI36H3ixDL6X1f6ySwLzOPdjqxBcYrDUnzXkTcTe9nhAJ4lt5y/AC+7w+ZXlqunlJJRNyZmX/d9Ty0MEXEvwPXZ+Z/z6gvAY5k5lg3M1t4vNJQ1be6noAWtPeA3xxQv7a16RLxmYbeFxEvzNYEXHMp5yLN8A3gUEQc44MfMf0t4Drgrs5mtQB5e0rvi4jXgPXAmZlNwD9n5qB/6UmXRERcQe+/S1hO7zM5BRzOzHc7ndgC45WG+j0OfCIzn5/ZEBFPXfrpSB/IzPeAp7uex0LnlYYkqcwH4ZKkMkNDklRmaEiSygwNSVLZ/wAVWbsAaUbA3AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax=final.Score.value_counts().plot(kind='bar')\n",
    "fig = ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ma', 'shan', \"shan't\"]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "stop = stopwords.words('english') #All the stopwords in English language\n",
    "#excluding some useful words from stop words list \n",
    "excluding = ['against','not','don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "             'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", \n",
    "             'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn',\n",
    "             \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "stop = [words for words in stop if words not in excluding]\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def striphtml(data):\n",
    "    p = re.compile('<.*?>')#Find this kind of pattern\n",
    "\n",
    "    return p.sub('',data) #Substitute nothing at the place of strings which matched the patterns\n",
    "\n",
    "def strippunc(data):\n",
    "    p = re.compile(r'[?|!|\\'|\"|#|.|,|)|(|\\|/|~|%|*]')\n",
    "    return p.sub('',data)\n",
    "strippunc(\"fsd*?~,,,( sdfsdfdsvv)#\")\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "snow = SnowballStemmer('english') #initialising the snowball stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hence in the Preprocessing phase we do the following in the order below:-\n",
    "\n",
    " ### 1. Begin by removing the html tags\n",
    "### 2. Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "### 3. Check if the word is made up of english letters and is not alpha-numeric\n",
    "### 4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
    "### 5. Convert the word to lowercase\n",
    "### 6. Remove Stopwords\n",
    "### 7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)\n",
    "### 8. After which we collect the words used to describe positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
      "['This', 'is', 'a', 'confection', 'that', 'has', 'been', 'around', 'a', 'few', 'centuries', 'It', 'is', 'a', 'light', 'pillowy', 'citrus', 'gelatin', 'with', 'nuts', '-', 'in', 'this', 'case', 'Filberts', 'And', 'it', 'is', 'cut', 'into', 'tiny', 'squares', 'and', 'then', 'liberally', 'coated', 'with', 'powdered', 'sugar', 'And', 'it', 'is', 'a', 'tiny', 'mouthful', 'of', 'heaven', 'Not', 'too', 'chewy', 'and', 'very', 'flavorful', 'I', 'highly', 'recommend', 'this', 'yummy', 'treat', 'If', 'you', 'are', 'familiar', 'with', 'the', 'story', 'of', 'CS', 'Lewis', 'The', 'Lion', 'The', 'Witch', 'and', 'The', 'Wardrobe', '-', 'this', 'is', 'the', 'treat', 'that', 'seduces', 'Edmund', 'into', 'selling', 'out', 'his', 'Brother', 'and', 'Sisters', 'to', 'the', 'Witch']\n",
      "================================> This\n",
      "Eliminated as it is a stopword\n",
      "================================> is\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> a\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> confection\n",
      "Selected: Stem Word-> b'confect'\n",
      "================================> that\n",
      "Eliminated as it is a stopword\n",
      "================================> has\n",
      "Eliminated as it is a stopword\n",
      "================================> been\n",
      "Eliminated as it is a stopword\n",
      "================================> around\n",
      "Selected: Stem Word-> b'around'\n",
      "================================> a\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> few\n",
      "Eliminated as it is a stopword\n",
      "================================> centuries\n",
      "Selected: Stem Word-> b'centuri'\n",
      "================================> It\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> is\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> a\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> light\n",
      "Selected: Stem Word-> b'light'\n",
      "================================> pillowy\n",
      "Selected: Stem Word-> b'pillowi'\n",
      "================================> citrus\n",
      "Selected: Stem Word-> b'citrus'\n",
      "================================> gelatin\n",
      "Selected: Stem Word-> b'gelatin'\n",
      "================================> with\n",
      "Eliminated as it is a stopword\n",
      "================================> nuts\n",
      "Selected: Stem Word-> b'nut'\n",
      "================================> -\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> in\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> this\n",
      "Eliminated as it is a stopword\n",
      "================================> case\n",
      "Selected: Stem Word-> b'case'\n",
      "================================> Filberts\n",
      "Selected: Stem Word-> b'filbert'\n",
      "================================> And\n",
      "Eliminated as it is a stopword\n",
      "================================> it\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> is\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> cut\n",
      "Selected: Stem Word-> b'cut'\n",
      "================================> into\n",
      "Eliminated as it is a stopword\n",
      "================================> tiny\n",
      "Selected: Stem Word-> b'tini'\n",
      "================================> squares\n",
      "Selected: Stem Word-> b'squar'\n",
      "================================> and\n",
      "Eliminated as it is a stopword\n",
      "================================> then\n",
      "Eliminated as it is a stopword\n",
      "================================> liberally\n",
      "Selected: Stem Word-> b'liber'\n",
      "================================> coated\n",
      "Selected: Stem Word-> b'coat'\n",
      "================================> with\n",
      "Eliminated as it is a stopword\n",
      "================================> powdered\n",
      "Selected: Stem Word-> b'powder'\n",
      "================================> sugar\n",
      "Selected: Stem Word-> b'sugar'\n",
      "================================> And\n",
      "Eliminated as it is a stopword\n",
      "================================> it\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> is\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> a\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> tiny\n",
      "Selected: Stem Word-> b'tini'\n",
      "================================> mouthful\n",
      "Selected: Stem Word-> b'mouth'\n",
      "================================> of\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> heaven\n",
      "Selected: Stem Word-> b'heaven'\n",
      "================================> Not\n",
      "Selected: Stem Word-> b'not'\n",
      "================================> too\n",
      "Eliminated as it is a stopword\n",
      "================================> chewy\n",
      "Selected: Stem Word-> b'chewi'\n",
      "================================> and\n",
      "Eliminated as it is a stopword\n",
      "================================> very\n",
      "Eliminated as it is a stopword\n",
      "================================> flavorful\n",
      "Selected: Stem Word-> b'flavor'\n",
      "================================> I\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> highly\n",
      "Selected: Stem Word-> b'high'\n",
      "================================> recommend\n",
      "Selected: Stem Word-> b'recommend'\n",
      "================================> this\n",
      "Eliminated as it is a stopword\n",
      "================================> yummy\n",
      "Selected: Stem Word-> b'yummi'\n",
      "================================> treat\n",
      "Selected: Stem Word-> b'treat'\n",
      "================================> If\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> you\n",
      "Eliminated as it is a stopword\n",
      "================================> are\n",
      "Eliminated as it is a stopword\n",
      "================================> familiar\n",
      "Selected: Stem Word-> b'familiar'\n",
      "================================> with\n",
      "Eliminated as it is a stopword\n",
      "================================> the\n",
      "Eliminated as it is a stopword\n",
      "================================> story\n",
      "Selected: Stem Word-> b'stori'\n",
      "================================> of\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> CS\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> Lewis\n",
      "Selected: Stem Word-> b'lewi'\n",
      "================================> The\n",
      "Eliminated as it is a stopword\n",
      "================================> Lion\n",
      "Selected: Stem Word-> b'lion'\n",
      "================================> The\n",
      "Eliminated as it is a stopword\n",
      "================================> Witch\n",
      "Selected: Stem Word-> b'witch'\n",
      "================================> and\n",
      "Eliminated as it is a stopword\n",
      "================================> The\n",
      "Eliminated as it is a stopword\n",
      "================================> Wardrobe\n",
      "Selected: Stem Word-> b'wardrob'\n",
      "================================> -\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> this\n",
      "Eliminated as it is a stopword\n",
      "================================> is\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> the\n",
      "Eliminated as it is a stopword\n",
      "================================> treat\n",
      "Selected: Stem Word-> b'treat'\n",
      "================================> that\n",
      "Eliminated as it is a stopword\n",
      "================================> seduces\n",
      "Selected: Stem Word-> b'seduc'\n",
      "================================> Edmund\n",
      "Selected: Stem Word-> b'edmund'\n",
      "================================> into\n",
      "Eliminated as it is a stopword\n",
      "================================> selling\n",
      "Selected: Stem Word-> b'sell'\n",
      "================================> out\n",
      "Eliminated as it is a stopword\n",
      "================================> his\n",
      "Eliminated as it is a stopword\n",
      "================================> Brother\n",
      "Selected: Stem Word-> b'brother'\n",
      "================================> and\n",
      "Eliminated as it is a stopword\n",
      "================================> Sisters\n",
      "Selected: Stem Word-> b'sister'\n",
      "================================> to\n",
      "Eliminated as it is a numerical value or character of lenght less than 2\n",
      "================================> the\n",
      "Eliminated as it is a stopword\n",
      "================================> Witch\n",
      "Selected: Stem Word-> b'witch'\n"
     ]
    }
   ],
   "source": [
    "## Just trying on 1 review to observe how it works\n",
    "\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[] # store words from +ve reviews here\n",
    "all_negative_words=[] # store words from -ve reviews here.\n",
    "s=''\n",
    "for sent in final['Text'][2:3].values: #Running only for 2nd review\n",
    "    filtered_sentence=[]\n",
    "    print(sent) #Each review\n",
    "    sent=striphtml(sent)# remove HTMl tags\n",
    "    sent=strippunc(sent)# remove Punctuation Symbols\n",
    "    print(sent.split())\n",
    "    for w in sent.split():\n",
    "        print(\"================================>\",w)\n",
    "        if((w.isalpha()) and (len(w)>2)):#If it is a numerical value or character of lenght less than 2    \n",
    "            if(w.lower() not in stop):# If it is a stopword\n",
    "                s=(snow.stem(w.lower())).encode('utf8') #Stemming the word using SnowBall Stemmer\n",
    "                print(\"Selected: Stem Word->\",s)\n",
    "                filtered_sentence.append(s)\n",
    "            else:\n",
    "                print(\"Eliminated as it is a stopword\")\n",
    "                continue\n",
    "        else:\n",
    "            print(\"Eliminated as it is a numerical value or character of lenght less than 2\")\n",
    "            continue \n",
    "#     print(filtered_sentence)\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    \n",
    "    final_string.append(str1)\n",
    "    #print(\"***********************************************************************\")\n",
    "    #print(\"Finally selected words from the review:\\n\",final_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568452\n",
      "Wall time: 15min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# With stemming operation\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[]                # store words from +ve reviews here\n",
    "all_negative_words=[]               # store words from -ve reviews here.\n",
    "s=''\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=striphtml(sent)                   # remove HTMl tags\n",
    "    sent=strippunc(sent)                    # remove Punctuation Symbols\n",
    "    for w in sent.split():\n",
    "        if((w.isalpha()) and (len(w)>2)):      #If it is a numerical value or character of lenght less than 2    \n",
    "            if(w.lower() not in stop):             # If it is a stopword\n",
    "                s=(snow.stem(w.lower())).encode('utf8') #Stemming the word using SnowBall Stemmer\n",
    "                filtered_sentence.append(s)\n",
    "                if (final['Score'].values)[i] == 'Positive': \n",
    "                    all_positive_words.append(s) #list of all words used to describe positive reviews\n",
    "                if(final['Score'].values)[i] == 'Negative':\n",
    "                    all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue \n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    final_string.append(str1)\n",
    "    #print(\"***********************************************************************\")\n",
    "    #print(\"Finally selected words from the review:\\n\",final_string)\n",
    "    i+=1\n",
    "print(len(final_string))\n",
    "final['CleanedText']=final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>CleanedText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>b'bought sever vital can dog food product foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>b'product arriv label jumbo salt peanutsth pea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>b'confect around centuri light pillowi citrus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>b'look secret ingredi robitussin believ found ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>b'great taffi great price wide assort yummi ta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "3                     3                       3      0  1307923200   \n",
       "4                     0                       0      1  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "                                         CleanedText  \n",
       "0  b'bought sever vital can dog food product foun...  \n",
       "1  b'product arriv label jumbo salt peanutsth pea...  \n",
       "2  b'confect around centuri light pillowi citrus ...  \n",
       "3  b'look secret ingredi robitussin believ found ...  \n",
       "4  b'great taffi great price wide assort yummi ta...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397916,) (170536,) (397916,) (170536,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "##Sorting data according to Time in ascending order for Time Based Splitting\n",
    "time_sorted_data = final.sort_values('Time', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
    "\n",
    "x = time_sorted_data['CleanedText'].values\n",
    "y = time_sorted_data['Score']\n",
    "\n",
    "# split the data set into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We cannot work with text directly when using machine learning algorithms.Instead, we need to convert the text to numbers.Algorithms take vectors of numbers as input, therefore we need to convert documents to fixed-length vectors of numbers.A simple and effective model for thinking about text documents in machine learning is called the Bag-of-Words Model, or BoW.\n",
    "### The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer : <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer :  (397916, 19100)\n",
      "the number of unique words : 19100\n"
     ]
    }
   ],
   "source": [
    "#Bag of Words (BoW)\n",
    "count_vect = CountVectorizer(min_df = 10) \n",
    "X_train_vec = count_vect.fit_transform(X_train)\n",
    "X_test_vec = count_vect.transform(X_test)\n",
    "print(\"the type of count vectorizer :\",type(X_train_vec))\n",
    "print(\"the shape of out text BOW vectorizer : \",X_train_vec.get_shape())\n",
    "print(\"the number of unique words :\", X_train_vec.get_shape()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler(with_mean=False)\n",
    "X_train_vec_standardized = sc.fit_transform(X_train_vec)\n",
    "X_test_vec_standardized = sc.transform(X_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV\n",
    "Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.\n",
    "### Randomized Search CV\n",
    "Random search is an approach to parameter tuning that will sample algorithm parameters from a random distribution (i.e. uniform) for a fixed number of iterations. A model is constructed and evaluated for each combination of parameters chosen.\n",
    "\n",
    "## SGD Classifier\n",
    "SGD Classifier implements regularised linear models with Stochastic Gradient Descent.So, what is SGD is doing is actually scanning through the all the training data points and for the first training data point it looks at the cost function, calculates the gradient to modify the model parameters so that only the first predicted value for training data point fits to the actual value better. Then it will go to the second training data point and again modify the previously calculated model parameters to only fit the second training data point better (ignoring the previous training data point). It will again do the same to the third training data point to modify the previously calculated training parameters so that only the third predicted training data point fits better to the actual value. This process continues until it covers all the m data points in the training data set. This concludes one iteration or epoch of the SGD. This entire process will again repeat till a solution is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with best parameters :\n",
      " SGDClassifier(alpha=0.1, average=False, class_weight=None, early_stopping=False,\n",
      "              epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "              learning_rate='optimal', loss='hinge', max_iter=1000,\n",
      "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
      "              random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "Accuracy of the model :  0.8928496036027583\n",
      "The optimal value of alpha(1/C) is :  0.1\n"
     ]
    }
   ],
   "source": [
    "#GridSearchCV Implementation\n",
    "# Importing libraries\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,precision_score,recall_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "Alpha = [0.0001,0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "param_grid = {'alpha': Alpha}\n",
    "model = GridSearchCV(SGDClassifier(), param_grid, scoring = 'f1_micro', cv=3 , n_jobs = -1,pre_dispatch=2)\n",
    "model.fit(X_train_vec_standardized, Y_train)\n",
    "print(\"Model with best parameters :\\n\",model.best_estimator_)\n",
    "print(\"Accuracy of the model : \",model.score(X_test_vec_standardized, Y_test))\n",
    "\n",
    "optimal_alpha = model.best_estimator_.alpha\n",
    "print(\"The optimal value of alpha(1/C) is : \",optimal_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Parameters\n",
      "Accuracy of the model at optimal hyperparameter alpha = 0.100000% is:  89.272060%\n",
      "f1 score value for   the model is: 0.8880480518468967\n",
      "precision score  for   the model is: 0.8888499216112815\n",
      "\n",
      "Train Parameters\n",
      "Accuracy of the model at optimal hyperparameter alpha = 0.100000% is:  90.637723%\n",
      "f1 score value for   the model is: 0.9023953126245627\n",
      "precision score  for   the model is: 0.9038421032616855\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "#confusion matrix,precision matrix,recall matrix,accuracy\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
    "sgd = SGDClassifier(alpha=optimal_alpha, n_jobs=-1)\n",
    "sgd.fit(X_train_vec_standardized,Y_train)\n",
    "print(\"Test Parameters\")\n",
    "Y_pred = sgd.predict(X_test_vec_standardized)\n",
    "Y_test_accuracy = accuracy_score(Y_test, Y_pred, normalize=True, sample_weight=None)*100\n",
    "print('Accuracy of the model at optimal hyperparameter alpha = %f%% is:  %f%%' % (optimal_alpha,Y_test_accuracy))\n",
    "f1score= f1_score(Y_test, Y_pred, average='weighted')\n",
    "print('f1 score value for   the model is: %s'% f1score)\n",
    "precisionscore=precision_score(Y_test, Y_pred,average='weighted')\n",
    "print('precision score  for   the model is: %s'% precisionscore)\n",
    "y_train_pred = sgd.predict(X_train_vec_standardized)\n",
    "print()\n",
    "Y_train_accuracy =accuracy_score(Y_train, y_train_pred, normalize=True, sample_weight=None)*100\n",
    "print(\"Train Parameters\")\n",
    "print('Accuracy of the model at optimal hyperparameter alpha = %f%% is:  %f%%' % (optimal_alpha,Y_train_accuracy))\n",
    "f1score= f1_score(Y_train, y_train_pred, average='weighted')\n",
    "print('f1 score value for   the model is: %s'% f1score)\n",
    "precisionscore=precision_score(Y_train, y_train_pred,pos_label='positive',average='weighted')\n",
    "print('precision score  for   the model is: %s'% precisionscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with best parameters :\n",
      " SGDClassifier(alpha=0.14604441595295203, average=False, class_weight=None,\n",
      "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
      "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
      "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "Accuracy of the model :  0.8912135854013228\n",
      "The optimal value of alpha(1/C) is :  0.14604441595295203\n",
      "\n",
      "Test Scores\n",
      "Accuracy of the model at optimal hyperparameter alpha = 0.146044% is:  89.116081%\n",
      "f1 score value for   the model is: 0.8856828748578753\n",
      "precision score  for   the model is: 0.8873289413335012\n",
      "\n",
      "Train Scores\n",
      "Accuracy of the model at optimal hyperparameter alpha = 0.146044% is:  90.348717%\n",
      "f1 score value for   the model is: 0.8987753411711767\n",
      "precision score  for   the model is: 0.9010244588093905\n"
     ]
    }
   ],
   "source": [
    "#Using Randomized Search CV to find best parameters\n",
    "# Load libraries\n",
    "\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Create regularization hyperparameter distribution using uniform distribution\n",
    "Alpha = uniform(loc=0, scale=1)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(alpha=Alpha)\n",
    "\n",
    "#Using RandomizedSearchCV\n",
    "model = RandomizedSearchCV(SGDClassifier(), hyperparameters, scoring = 'f1_micro', cv=3 , n_jobs = -1,pre_dispatch=2)\n",
    "model.fit(X_train_vec_standardized, Y_train)\n",
    "print(\"Model with best parameters :\\n\",model.best_estimator_)\n",
    "print(\"Accuracy of the model : \",model.score(X_test_vec_standardized, Y_test))\n",
    "\n",
    "optimal_alpha = model.best_estimator_.alpha\n",
    "print(\"The optimal value of alpha(1/C) is : \",optimal_alpha)\n",
    "print()\n",
    "\n",
    "#precision matrix,recall matrix,accuracy\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
    "sgd = SGDClassifier(alpha=optimal_alpha, n_jobs=-1)\n",
    "sgd.fit(X_train_vec_standardized,Y_train)\n",
    "Y_pred = sgd.predict(X_test_vec_standardized)\n",
    "print(\"Test Scores\")\n",
    "\n",
    "Y_test_accuracy = accuracy_score(Y_test, Y_pred, normalize=True, sample_weight=None)*100\n",
    "print('Accuracy of the model at optimal hyperparameter alpha = %f%% is:  %f%%' % (optimal_alpha,Y_test_accuracy))\n",
    "f1score= f1_score(Y_test, Y_pred, average='weighted')\n",
    "print('f1 score value for   the model is: %s'% f1score)\n",
    "precisionscore=precision_score(Y_test, Y_pred,average='weighted')\n",
    "print('precision score  for   the model is: %s'% precisionscore)\n",
    "print()\n",
    "print(\"Train Scores\")\n",
    "y_train_pred = sgd.predict(X_train_vec_standardized)\n",
    "Y_train_accuracy =accuracy_score(Y_train, y_train_pred, normalize=True, sample_weight=None)*100\n",
    "#plot_confusion_matrix(Y_train, y_train_pred)\n",
    "print('Accuracy of the model at optimal hyperparameter alpha = %f%% is:  %f%%' % (optimal_alpha,Y_train_accuracy))\n",
    "f1score= f1_score(Y_train, y_train_pred, average='weighted')\n",
    "print('f1 score value for   the model is: %s'% f1score)\n",
    "precisionscore=precision_score(Y_train, y_train_pred,average='weighted')\n",
    "print('precision score  for   the model is: %s'% precisionscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Model Performance on Test Data************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Reviews</th>\n",
       "      <th>Original Score</th>\n",
       "      <th>Model Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'lucki receiv gift packag lake champlain hot ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'admit dont know much babi dont one receiv ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'like amazon method regular ship favorit coff...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'ever fresh open gourmet coffe wonder happen ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'saw bar wyom somewher buddi love hotnspici p...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b'make waffl mix flour tast great healthi grap...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b'amaz much got small price best coffe ive tas...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b'use coconut oil past day happi result first ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b'ill start note love almond eat almond everi ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b'dog favorit flavor blue buffalo biscuit reas...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Test Reviews  Original Score  \\\n",
       "0  b'lucki receiv gift packag lake champlain hot ...               1   \n",
       "1  b'admit dont know much babi dont one receiv ba...               1   \n",
       "2  b'like amazon method regular ship favorit coff...               1   \n",
       "3  b'ever fresh open gourmet coffe wonder happen ...               0   \n",
       "4  b'saw bar wyom somewher buddi love hotnspici p...               1   \n",
       "5  b'make waffl mix flour tast great healthi grap...               1   \n",
       "6  b'amaz much got small price best coffe ive tas...               1   \n",
       "7  b'use coconut oil past day happi result first ...               1   \n",
       "8  b'ill start note love almond eat almond everi ...               1   \n",
       "9  b'dog favorit flavor blue buffalo biscuit reas...               1   \n",
       "\n",
       "   Model Prediction  \n",
       "0                 1  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 1  \n",
       "5                 1  \n",
       "6                 1  \n",
       "7                 1  \n",
       "8                 1  \n",
       "9                 1  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_op = (sgd.predict(X_test_vec))\n",
    "tup=list(zip(X_test,Y_test,test_op))\n",
    "print('********************Model Performance on Test Data************************')\n",
    "pd.DataFrame(tup, columns=['Test Reviews','Original Score','Model Prediction']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
